{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Text processing\n",
    "\n",
    "We will create the pipline of text preprocessing\n",
    "\n",
    "# 1. 1 Normalization\n",
    "\n",
    "The first step is normalisation.\n",
    "It might include:\n",
    "* converting all letters to lower or upper case\n",
    "* converting numbers into words or removing numbers\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing white spaces\n",
    "* expanding abbreviations\n",
    "\n",
    "In this exercise it would be ok to have a lowercase text without specific characters and digits and without unnecessery space symbols.\n",
    "\n",
    "How neural networks could be implemented for text normalization?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "# normalize text\n",
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = ' '.join(re.findall(r'[\\w*]+', text))\n",
    "    return text\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"\"\"This sentence is going to be lemmatized. Borrowed from Latin per sē (“by itself”), from per (“by, through”) and sē (“itself, himself, herself, themselves”)\"\"\"\n",
    "text = normalize(text)\n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.2 Tokenize\n",
    "Use nltk tokenizer to tokenize the text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenize text using nltk lib\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = tokenize(text)\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.3 Lemmatization\n",
    "What is the difference between stemming and lemmatization?\n",
    "\n",
    "[Optional reading](https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmatization(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lemmed = lemmatization(tokens)\n",
    "print(lemmed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.4 Stop words\n",
    "The next step is to remove stop words. Take the list of stop words from nltk."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_word(tokens):\n",
    "    return [word for word in tokens if word not in stopwords.words('english')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean = remove_stop_word(lemmed)\n",
    "print(clean)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.5 Pipeline\n",
    "Run a complete pipeline inone function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return remove_stop_word(lemmatization(tokenize(normalize(text))))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clean = preprocess(text)\n",
    "print(clean)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SoundEx algo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def soundex(word):\n",
    "  first_letter = word[0]\n",
    "  word = word[1:]\n",
    "  for i in range(len(word)):\n",
    "    c = word[i]\n",
    "    if c in 'aeiouhwy':\n",
    "      word = word.replace(c, '0')\n",
    "    elif c in 'bfpv':\n",
    "      word = word.replace(c, '1')\n",
    "    elif c in 'cgjkqsxz':\n",
    "      word = word.replace(c, '2')\n",
    "    elif c in 'dt':\n",
    "      word = word.replace(c, '3')\n",
    "    elif c == 'l':\n",
    "      word = word.replace(c, '4')\n",
    "    elif c in 'mn':\n",
    "      word = word.replace(c, '5')\n",
    "    elif c == 'r':\n",
    "      word = word.replace(c, '6')\n",
    "  if len(word) > 0:\n",
    "    res = word[0]\n",
    "  else:\n",
    "    res = ''\n",
    "  for i in range(1, len(word)):\n",
    "    if word[i] != word[i - 1]:\n",
    "      res += word[i]\n",
    "  res = res.replace('0', '')\n",
    "  if len(res) < 3:\n",
    "    c = 3 - len(res)\n",
    "    res = '0' * c + res\n",
    "  return first_letter + res[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Levenstein Distance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def levenstein_distance(a, b):\n",
    "  d = [[0 for j in range(len(b))] for i in range(len(a))]\n",
    "  for i in range(1, len(a)):\n",
    "    d[i][0] = i\n",
    "  for j in range(1, len(b)):\n",
    "    d[0][j] = j\n",
    "  for i in range(1, len(a)):\n",
    "    for j in range(1, len(b)):\n",
    "      if a[i] == b[j]:\n",
    "        x = 0\n",
    "      else:\n",
    "        x = 1\n",
    "      d[i][j] = min(d[i - 1][j - 1] + x, d[i - 1][j] + 1, d[i][j - 1] + 1)\n",
    "  return d[len(a) - 1][len(b) - 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Collection\n",
    "\n",
    "Download Reuters data from here:\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz\n",
    "\n",
    "Read data description here:\n",
    "https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection\n",
    "\n",
    "The function should return a list of strings - raw texts. Remove html tags using bs4 package."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "\n",
    "def get_collection():\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz'\n",
    "    resp = requests.get(url, timeout=5)\n",
    "    with open('data.tar.gz', \"wb\") as output:\n",
    "      output.write(resp.content)\n",
    "    if not os.path.exists('dataset'):\n",
    "      os.mkdir('dataset')\n",
    "      tar = tarfile.open('data.tar.gz', \"r:gz\")\n",
    "      tar.extractall('dataset')\n",
    "      tar.close()\n",
    "\n",
    "    collection = []\n",
    "    i = 0\n",
    "    for filename in os.listdir('dataset'):\n",
    "      if '.sgm' in filename:\n",
    "        with open(os.path.join('dataset', filename), \"rb\") as input:\n",
    "          soup = BeautifulSoup(input.read())\n",
    "          texts = soup.find_all(\"text\")\n",
    "          for news in texts:\n",
    "            collection.append((news.text, i))\n",
    "            i += 1\n",
    "    return collection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collection = get_collection()\n",
    "print(len(collection))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(collection[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prefix tree dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add(w, prefix_dict):\n",
    "  d = prefix_dict\n",
    "  for c in w:\n",
    "    if not c in d.keys():\n",
    "      d[c] = {}  \n",
    "    d = d[c]\n",
    "  d['!'] = 1\n",
    "\n",
    "\n",
    "def add_text(text):\n",
    "  global prefix_dict, r_prefix_dict\n",
    "  for w in text:\n",
    "    add(w, prefix_dict)\n",
    "    add(w[::-1], r_prefix_dict)\n",
    "\n",
    "\n",
    "def traverse(tree, word):\n",
    "  res = []\n",
    "  keys = tree.keys()\n",
    "  if '!' in keys:\n",
    "    res.append(word)\n",
    "  for c in keys:\n",
    "    if c == '!':\n",
    "      continue\n",
    "    res += traverse(tree[c], word + c)\n",
    "  return res\n",
    "\n",
    "\n",
    "def find_words(pref, tree):\n",
    "  d = tree\n",
    "  res = []\n",
    "  for c in pref:\n",
    "    if c in d.keys():\n",
    "      d = d[c]\n",
    "    else:\n",
    "      return res\n",
    "  if '!' in d.keys():\n",
    "    res.append(pref)\n",
    "  for c in d.keys():\n",
    "    if c == '!':\n",
    "      continue\n",
    "    res += traverse(d[c], pref + c)\n",
    "  return res\n",
    "\n",
    "prefix_dict = {}\n",
    "r_prefix_dict = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Inverted index\n",
    "You will work with the boolean search model. Construct a dictionary which maps words to the postings.  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_index(collection):\n",
    "    inverted_index = {}\n",
    "    for element in collection:\n",
    "      text = preprocess(element[0])\n",
    "      posting = element[1]\n",
    "      if posting % 1000 == 0:\n",
    "        print(f\"Posting [{posting}] processed\")\n",
    "      for word in text:\n",
    "        if not word in inverted_index.keys():\n",
    "          inverted_index[word] = set()\n",
    "        inverted_index[word].add(posting)\n",
    "    for key in inverted_index.keys():\n",
    "      inverted_index[key] = list(inverted_index[key])\n",
    "    return inverted_index\n",
    "\n",
    "def make_soundex_index(collection):\n",
    "    inverted_index = {}\n",
    "    soundex_dict = {}\n",
    "    for element in collection:\n",
    "      text = remove_stop_word(tokenize(normalize(element[0])))\n",
    "      add_text(text)\n",
    "      posting = element[1]\n",
    "      if posting % 1000 == 0:\n",
    "        print(f\"Posting [{posting}] processed\")\n",
    "      for word in text:\n",
    "        soundexed = soundex(word)\n",
    "        if not soundexed in inverted_index.keys():\n",
    "          inverted_index[soundexed] = set()\n",
    "        if not soundexed in soundex_dict.keys():\n",
    "          soundex_dict[soundexed] = set()\n",
    "        inverted_index[soundexed].add(posting)\n",
    "        soundex_dict[soundexed].add(word)\n",
    "    for key in inverted_index.keys():\n",
    "      inverted_index[key] = list(inverted_index[key])\n",
    "      soundex_dict[key] = list(soundex_dict[key])\n",
    "    return inverted_index, soundex_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "if os.path.exists(\"index.json\"):\n",
    "    index = json.loads(open(\"index.json\", \"r\").read())\n",
    "else:\n",
    "    index = make_index(collection)\n",
    "    json.dump(index, open(\"index.json\",\"w\"))\n",
    "if os.path.exists(\"soundex_index.json\"):\n",
    "    soundex_index = json.loads(open(\"soundex_index.json\", \"r\").read())\n",
    "    soundex_dict = json.loads(open(\"soundex_dict.json\", \"r\").read())\n",
    "else:\n",
    "    soundex_index, soundex_dict = make_soundex_index(collection)\n",
    "    json.dump(soundex_index, open(\"soundex_index.json\",\"w\"))\n",
    "    json.dump(soundex_dict, open(\"soundex_dict.json\",\"w\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Query processing\n",
    "\n",
    "Using given search query, find all relevant documents. In binary model the relevant document is the one which contains all words from the query.\n",
    "\n",
    "Return the list of relevant documents indexes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def retrieve_documents(words):\n",
    "  \"\"\"Retrieve all documents containing either of present words\"\"\"\n",
    "  words = lemmatization(words)\n",
    "  res = set()\n",
    "  for w in words:\n",
    "    res = res | set(index[w])\n",
    "  return res\n",
    "\n",
    "\n",
    "def find_alternatives(word):\n",
    "  \"\"\"\n",
    "  Use soundex to find similar words, \n",
    "  retrieve 0.1 of found words sorted by Levenstein distance\n",
    "  \"\"\"\n",
    "  s = soundex(word)\n",
    "  if s in soundex_dict.keys():\n",
    "    soundex_list = soundex_dict[s]\n",
    "  else:\n",
    "    soundex_list = []\n",
    "  distances = []\n",
    "  for w in soundex_list:\n",
    "    distances.append(levenstein_distance(word, w))\n",
    "  z = sorted(zip(distances, soundex_list))\n",
    "  distances, soundex_list = zip(*z)\n",
    "  if len(soundex_list) > 0:\n",
    "    n = ceil(len(soundex_list) * 0.1)\n",
    "    return soundex_list[:n]\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "\n",
    "def reverse_words(words):\n",
    "  for i in range(len(words)):\n",
    "    words[i] = words[i][::-1]\n",
    "  return words\n",
    "\n",
    "\n",
    "def search(query):\n",
    "    query = preprocess(query)\n",
    "    relevant_documents = None\n",
    "    for i in range(len(query)):\n",
    "      if '*' in query[i]:\n",
    "        if query[i].count('*') > 1: # Search with several stars is not implemented\n",
    "          continue\n",
    "        x = query[i].find('*')\n",
    "        if not relevant_documents:\n",
    "          relevant_documents = retrieve_documents(\n",
    "              set(find_words(query[i][:x], prefix_dict)) & # Find and lemmatize words that match prefix before '*'\n",
    "              set(reverse_words(find_words(query[i][x + 1:][::-1], r_prefix_dict))) # Find and lemmatize words that match suffix after '*'\n",
    "          )\n",
    "        else:\n",
    "          relevant_documents = relevant_documents & \\\n",
    "            retrieve_documents(\n",
    "                set(find_words(query[i][:x], prefix_dict)) & # Find and lemmatize words that match prefix before '*'\n",
    "                set(reverse_words(find_words(query[i][x + 1:][::-1], r_prefix_dict))) # Find and lemmatize words that match suffix after '*'\n",
    "            )\n",
    "      else:\n",
    "        if not relevant_documents:\n",
    "          if query[i] in index.keys():\n",
    "            relevant_documents = set(index[query[i]])\n",
    "          else:\n",
    "            relevant_documents = retrieve_documents(find_alternatives(query[i]))\n",
    "        else:\n",
    "          if query[i] in index.keys():\n",
    "            relevant_documents = relevant_documents & set(index[query[i]])\n",
    "          else:\n",
    "            relevant_documents = relevant_documents & \\\n",
    "            retrieve_documents(find_alternatives(query[i]))\n",
    "    return list(relevant_documents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = 'moskow ex*ess'\n",
    "relevant = search(query)\n",
    "print(len(relevant))\n",
    "print(relevant)\n",
    "if len(relevant) > 0:\n",
    "  print(collection[relevant[0]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}