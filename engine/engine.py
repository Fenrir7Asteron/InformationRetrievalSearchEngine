# -*- coding: utf-8 -*-
"""Copy of Search Engine template

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ig-2brxtXnRwWxA8uzVio2HwS-qthzH1

# 1. Text processing

We will create the pipline of text preprocessing

# 1. 1 Normalization

The first step is normalisation.
It might include:
* converting all letters to lower or upper case
* converting numbers into words or removing numbers
* removing punctuations, accent marks and other diacritics
* removing white spaces
* expanding abbreviations

In this exercise it would be ok to have a lowercase text without specific characters and digits and without unnecessery space symbols.

How neural networks could be implemented for text normalization?
"""

import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import requests
import tarfile
import os
from bs4 import BeautifulSoup
import json
import utils.shared as shared

# normalize text
from utils.list_operations import merge_lists
from utils.storage import store_document, append_to_listing


def normalize(text):
    text = text.lower()
    text = ' '.join(re.findall(r'[\w*]+', text))
    return text


"""# 1.2 Tokenize
Use nltk tokenizer to tokenize the text
"""

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

# tokenize text using nltk lib
def tokenize(text):
    return word_tokenize(text)


"""# 1.3 Lemmatization
What is the difference between stemming and lemmatization?

[Optional reading](https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8)
"""


def lemmatization(tokens):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(token) for token in tokens]


"""# 1.4 Stop words
The next step is to remove stop words. Take the list of stop words from nltk.
"""


def remove_stop_word(tokens):
    return [word for word in tokens if word not in stopwords.words('english')]


"""# 1.5 Pipeline
Run a complete pipeline inone function.
"""


def preprocess(text):
    return remove_stop_word(lemmatization(tokenize(normalize(text))))


"""# SoundEx algo"""


def soundex(word):
    first_letter = word[0]
    word = word[1:]
    for i in range(len(word)):
        c = word[i]
        if c in 'aeiouhwy':
            word = word.replace(c, '0')
        elif c in 'bfpv':
            word = word.replace(c, '1')
        elif c in 'cgjkqsxz':
            word = word.replace(c, '2')
        elif c in 'dt':
            word = word.replace(c, '3')
        elif c == 'l':
            word = word.replace(c, '4')
        elif c in 'mn':
            word = word.replace(c, '5')
        elif c == 'r':
            word = word.replace(c, '6')
    if len(word) > 0:
        res = word[0]
    else:
        res = ''
    for i in range(1, len(word)):
        if word[i] != word[i - 1]:
            res += word[i]
    res = res.replace('0', '')
    if len(res) < 3:
        c = 3 - len(res)
        res = '0' * c + res
    return first_letter + res[:3]


"""# Levenstein Distance"""


def levenstein_distance(a, b):
    d = [[0 for j in range(len(b))] for i in range(len(a))]
    for i in range(1, len(a)):
        d[i][0] = i
    for j in range(1, len(b)):
        d[0][j] = j
    for i in range(1, len(a)):
        for j in range(1, len(b)):
            if a[i] == b[j]:
                x = 0
            else:
                x = 1
            d[i][j] = min(d[i - 1][j - 1] + x, d[i - 1][j] + 1, d[i][j - 1] + 1)
    return d[len(a) - 1][len(b) - 1]


"""# 2. Collection

Download Reuters data from here:
https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz

Read data description here:
https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection

The function should return a list of strings - raw texts. Remove html tags using bs4 package.
"""


def download_collection():
    if not os.path.exists('dataset'):
        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz'
        resp = requests.get(url, timeout=5)
        with open('data.tar.gz', "wb") as output:
            output.write(resp.content)
        os.mkdir('dataset')
        tar = tarfile.open('data.tar.gz', "r:gz")
        tar.extractall('dataset')
        tar.close()


"""# Prefix tree dictionary"""


def add(w, prefix_dict):
    d = prefix_dict
    for c in w:
        if not c in d.keys():
            d[c] = {}
        d = d[c]
    d['!'] = 1


def add_text(text):
    for w in text:
        add(w, shared.prefix_dict)
        add(w[::-1], shared.r_prefix_dict)


def traverse(tree, word):
    res = []
    keys = tree.keys()
    if '!' in keys:
        res.append(word)
    for c in keys:
        if c == '!':
            continue
        res += traverse(tree[c], word + c)
    return res


def find_words(pref, tree):
    d = tree
    res = []
    for c in pref:
        if c in d.keys():
            d = d[c]
        else:
            return res
    if '!' in d.keys():
        res.append(pref)
    for c in d.keys():
        if c == '!':
            continue
        res += traverse(d[c], pref + c)
    return preprocess(' '.join(res))


def get_trees():
    if not os.path.exists("../prefix_dict.json"):
        prefix_dict = {}
        r_prefix_dict = {}
    else:
        prefix_dict = json.loads(open("../prefix_dict.json", "r").read())
        r_prefix_dict = json.loads(open("../r_prefix_dict.json", "r").read())
    return prefix_dict, r_prefix_dict


"""
Query processing
"""

from math import ceil


def retrieve_documents(words, index):
    """Retrieve all documents containing either of present words"""
    res = set()
    for w in words:
        res = res | set(index[w])
    return res


def find_alternatives(word, soundex_dict):
    """
  Use soundex to find similar words, 
  retrieve 0.1 of found words sorted by Levenstein distance
  """
    s = soundex(word)
    if s in soundex_dict.keys():
        soundex_list = soundex_dict[s]
    else:
        soundex_list = []
    distances = []
    for w in soundex_list:
        distances.append(levenstein_distance(word, w))
    z = sorted(zip(distances, soundex_list))
    distances, soundex_list = zip(*z)
    if len(soundex_list) > 0:
        n = ceil(len(soundex_list) * 0.1)
        return preprocess(' '.join(soundex_list[:n]))
    else:
        return []


def reverse_words(words):
    for i in range(len(words)):
        words[i] = words[i][::-1]
    return words


def search(query):
    query = preprocess(query)
    relevant_documents = None
    for i in range(len(query)):
        if '*' in query[i]:
            if query[i].count('*') > 1:  # Search with several stars is not implemented
                continue
            x = query[i].find('*')
            if not relevant_documents:
                relevant_documents = retrieve_documents(
                    set(find_words(query[i][:x],
                                   shared.prefix_dict)) &  # Find and lemmatize words that match prefix before '*'
                    set(reverse_words(find_words(query[i][x + 1:][::-1], shared.r_prefix_dict)))
                    # Find and lemmatize words that match suffix after '*'
                    , index)
            else:
                relevant_documents = relevant_documents & \
                                     retrieve_documents(
                                         set(find_words(query[i][:x],
                                                        shared.prefix_dict)) &  # Find and lemmatize words that match prefix before '*'
                                         set(reverse_words(find_words(query[i][x + 1:][::-1], shared.r_prefix_dict)))
                                         # Find and lemmatize words that match suffix after '*'
                                         , index)
        else:
            if not relevant_documents:
                if query[i] in index.keys():
                    relevant_documents = set(index[query[i]])
                else:
                    relevant_documents = retrieve_documents(find_alternatives(query[i], shared.soundex_dict), index)
            else:
                if query[i] in index.keys():
                    relevant_documents = relevant_documents & set(index[query[i]])
                else:
                    relevant_documents = relevant_documents & \
                                         retrieve_documents(find_alternatives(query[i], shared.soundex_dict), index)
    return list(relevant_documents)


last = 0
doc_id = 0


def crawl():
    global last, doc_id

    files = os.listdir('dataset')
    while last < len(files):
        print(last)
        filename = files[last]
        if '.sgm' not in filename:
            last += 1
            continue
        with open(os.path.join('dataset', filename), "rb") as input:
            soup = BeautifulSoup(input.read())
            texts = soup.find_all("text")
            for news in texts:
                store_document(doc_id, str(news.title)[7:-8], news.text)
                update_indices(str(news.title)[7:-8] + ' ' + news.text, doc_id)
                doc_id += 1
        last += 1
        break


def update_indices(text: str, doc_id: int):
    text_lemmatized = preprocess(text)
    text_non_lemmatized = remove_stop_word(tokenize(normalize(text)))
    for word in text_lemmatized:
        # Update aux index
        append_to_listing(word, doc_id, 'AUX')
    # Update prefix trees
    add_text(text_non_lemmatized)
    for word in text_non_lemmatized:
        # Update soundex index
        soundexed = soundex(word)

        if not soundexed in shared.soundex_dict.keys():
            shared.soundex_dict[soundexed] = []
        merge_lists(shared.soundex_dict[soundexed], word)
